{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9W3_HAaxIwf"
      },
      "source": [
        "# Install Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "trNGcwfixHTd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /home/wsl/.local/lib/python3.10/site-packages (0.26.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /home/wsl/.local/lib/python3.10/site-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /home/wsl/.local/lib/python3.10/site-packages (from gym[accept-rom-license,atari]) (1.25.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/wsl/.local/lib/python3.10/site-packages (from gym[accept-rom-license,atari]) (3.0.0)\n",
            "Requirement already satisfied: ale-py~=0.8.0 in /home/wsl/.local/lib/python3.10/site-packages (from gym[accept-rom-license,atari]) (0.8.1)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /home/wsl/.local/lib/python3.10/site-packages (from gym[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: importlib-resources in /home/wsl/.local/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (6.4.0)\n",
            "Requirement already satisfied: typing-extensions in /home/wsl/.local/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]) (4.11.0)\n",
            "Requirement already satisfied: tqdm in /home/wsl/.local/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.66.4)\n",
            "Requirement already satisfied: click in /home/wsl/.local/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /home/wsl/.local/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.31.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /home/wsl/.local/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (0.6.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/wsl/.local/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2024.2.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/wsl/.local/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wsl/.local/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wsl/.local/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.3.2)\n",
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "!pip install gym[atari,accept-rom-license]\n",
        "ATARI_LIVES_KEY = 'lives'\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2ER7qNQqV0f"
      },
      "source": [
        "#コード"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLBi7b94uClV"
      },
      "source": [
        "## ActorCritic Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Km9yoc_e6LoE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "class RunningMean(tf.keras.metrics.Metric):\n",
        "  def __init__(self, gamma, name):\n",
        "    super().__init__(name=name)\n",
        "    self.gamma = gamma\n",
        "    self.v = self.add_weight('v', initializer='zeros', dtype=tf.float32)\n",
        "    self.count = self.add_weight('count', initializer='zeros', dtype=tf.float32)\n",
        "\n",
        "  def update_state(self, value):\n",
        "      self.v.assign( value + self.gamma*(self.v-value) )\n",
        "      return self.count.assign_add(1)\n",
        "\n",
        "  def result(self):\n",
        "      return self.v/(1-self.gamma**self.count)\n",
        "\n",
        "\n",
        "class ActorCritic(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, network, action_space,  dims=0, value_coef=0.5, entropy_coef=0.01, norm_advs=1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.action_space = action_space\n",
        "\n",
        "\n",
        "        self.norm_advs=norm_advs\n",
        "        self.value_coef = value_coef\n",
        "\n",
        "        self.entropy_coef = entropy_coef\n",
        "\n",
        "        self.network = network\n",
        "\n",
        "        self.to_values = layers.Dense(1, kernel_initializer=tf.initializers.Orthogonal(1.0) )\n",
        "        self.to_actions = layers.Dense(action_space, kernel_initializer=tf.initializers.Orthogonal(0.01) )\n",
        "\n",
        "        self.tracker_loss = RunningMean(0.99, name=\"loss\")\n",
        "        self.tracker_entropy = RunningMean(0.99, name=\"entropy\")\n",
        "        self.tracker_policy_loss = RunningMean(0.99, name=\"policy_loss\")\n",
        "        self.tracker_value_loss = RunningMean(0.99, name=\"value_loss\")\n",
        "\n",
        "\n",
        "    def sample(self, states):\n",
        "        # values, policy = self(states, training=False)\n",
        "        # return values.numpy(), policy.numpy()\n",
        "        pred = self.predict_on_batch(states)\n",
        "        return pred[0], pred[1]\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, training=None):\n",
        "        x = self.network(x, training=training )\n",
        "        actions = self.to_actions(x)\n",
        "        values = self.to_values(x)\n",
        "        return values, actions\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        n = self.optimizer.iterations\n",
        "        try:\n",
        "            advantages = data[0][2]\n",
        "            # Normalize advantages\n",
        "            if self.norm_advs==1:\n",
        "                advantages = advantages / (tf.math.reduce_std(advantages)+1e-8)\n",
        "            elif self.norm_advs==2:\n",
        "                advantages = (advantages - tf.math.reduce_mean(advantages))/ (tf.math.reduce_std(advantages)+1e-8)\n",
        "\n",
        "            results = self._train_step((data[0][0],data[0][1],advantages,data[0][3],data[0][4]),\n",
        "                                       data[1], self.value_coef, self.entropy_coef)\n",
        "\n",
        "        except Exception as e:\n",
        "            raise e\n",
        "\n",
        "        loss = results['loss']\n",
        "        policy_loss = results['policy_loss']\n",
        "        value_loss = results['value_loss']\n",
        "        entropy = results['entropy']\n",
        "\n",
        "        self.tracker_loss.update_state(loss)\n",
        "        self.tracker_entropy.update_state(entropy)\n",
        "        self.tracker_policy_loss.update_state(policy_loss)\n",
        "        self.tracker_value_loss.update_state(value_loss)\n",
        "\n",
        "        metrics = {\n",
        "            \"loss\": self.tracker_loss.result(),\n",
        "            \"value_loss\": self.tracker_value_loss.result(),\n",
        "            \"policy_loss\": self.tracker_policy_loss.result(),\n",
        "            \"entropy\": self.tracker_entropy.result(),\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _train_step(self, X, Y, value_coef, entropy_coef):\n",
        "        print('!!')\n",
        "        return\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxauNVPuFj04"
      },
      "source": [
        "### A2C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "CSf_5EojFmJZ"
      },
      "outputs": [],
      "source": [
        "class A2C(ActorCritic):\n",
        "    def __init__(self, *args, clip_global_norm=0.5, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.clip_global_norm = clip_global_norm\n",
        "\n",
        "    @tf.function\n",
        "    def _train_step(self, X, Y, value_coef,  entropy_coef):\n",
        "\n",
        "        states = X[0]\n",
        "        actions = X[1]\n",
        "\n",
        "        rewards = Y\n",
        "\n",
        "        EPS = 1e-8\n",
        "        def compute_logprob(logits, actions):\n",
        "            probs = tf.nn.softmax(logits)\n",
        "            actions_onehot = tf.one_hot(actions, self.action_space, dtype=tf.float32)\n",
        "            return tf.math.log(tf.reduce_sum(actions_onehot * probs, axis=1) + EPS)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            values, logits = self(states, training=True)\n",
        "            values = tf.squeeze(values)\n",
        "\n",
        "            advantages = rewards - values\n",
        "\n",
        "            action_log_probs = compute_logprob(logits, actions)\n",
        "\n",
        "            action_probs = tf.nn.softmax(logits)\n",
        "\n",
        "            policy_loss = action_log_probs * tf.stop_gradient(advantages)\n",
        "\n",
        "            entropy = -1 * tf.reduce_sum( action_probs * tf.math.log(action_probs + EPS), axis=1, keepdims=True)\n",
        "\n",
        "            value_loss = advantages ** 2\n",
        "\n",
        "            policy_loss = tf.reduce_mean(policy_loss)\n",
        "            value_loss = tf.reduce_mean(value_loss)\n",
        "            entropy = tf.reduce_mean(entropy)\n",
        "            loss  = value_coef * value_loss\n",
        "            loss -= policy_loss + (entropy_coef * entropy)\n",
        "\n",
        "        grads = tape.gradient(loss, self.trainable_variables)\n",
        "        grads, _ = tf.clip_by_global_norm(grads, self.clip_global_norm)\n",
        "        self.optimizer.apply_gradients( zip(grads, self.trainable_variables))\n",
        "\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"value_loss\": value_loss,\n",
        "            \"policy_loss\": policy_loss,\n",
        "            \"entropy\": entropy,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csuiO4-DFm3h"
      },
      "source": [
        "### PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "EXAa8KHbFrMo"
      },
      "outputs": [],
      "source": [
        "class PPO(ActorCritic):\n",
        "    def __init__(self, *args, pi_clip_range=0.2, v_clip_range=None, clip_global_norm=0.5,**kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.pi_clip_range = pi_clip_range\n",
        "        self.v_clip_range = v_clip_range\n",
        "        self.clip_global_norm = clip_global_norm\n",
        "\n",
        "    @tf.function\n",
        "    def _train_step(self, X, Y,value_coef,  entropy_coef):\n",
        "        states = X[0]\n",
        "        actions = X[1]\n",
        "        advantages = tf.squeeze(X[2])\n",
        "        old_logits = X[3]\n",
        "        old_values = X[4]\n",
        "        rewards = tf.squeeze(Y)\n",
        "\n",
        "        EPS = 1e-8\n",
        "\n",
        "        def compute_logprob(logits, actions):\n",
        "            probs = tf.nn.softmax(logits)\n",
        "            actions_onehot = tf.one_hot(actions, self.action_space, dtype=tf.float32)\n",
        "            return tf.math.log(tf.reduce_sum(actions_onehot * probs, axis=1) + EPS)\n",
        "\n",
        "\n",
        "        old_logprob = compute_logprob(old_logits, actions)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            values, logits = self(states, training=True)\n",
        "\n",
        "            action_probs = tf.nn.softmax(logits)\n",
        "\n",
        "            new_logprob = compute_logprob(logits, actions)\n",
        "\n",
        "            values = tf.squeeze(values)\n",
        "\n",
        "            value_loss = (values-rewards) ** 2\n",
        "            if self.v_clip_range is not None:\n",
        "                values_clipped = tf.clip_by_value(values, old_values-self.v_clip_range, old_values+self.v_clip_range)\n",
        "                value_loss_clipped = (values_clipped - rewards)**2\n",
        "                value_loss = tf.maximum( value_loss_clipped, value_loss )\n",
        "\n",
        "\n",
        "            entropy = -1 * tf.reduce_sum( action_probs * tf.math.log(action_probs + EPS), axis=1, keepdims=True)\n",
        "\n",
        "            ratio = tf.exp(new_logprob - old_logprob)\n",
        "            ratio_clipped = tf.clip_by_value(ratio, 1 - self.pi_clip_range, 1 + self.pi_clip_range)\n",
        "            loss_unclipped = ratio * advantages\n",
        "            loss_clipped = ratio_clipped * advantages\n",
        "            policy_loss = tf.minimum(loss_unclipped, loss_clipped)\n",
        "\n",
        "            policy_loss = tf.reduce_mean(policy_loss)\n",
        "            value_loss = tf.reduce_mean(value_loss)\n",
        "            entropy = tf.reduce_mean(entropy)\n",
        "            loss  = value_coef * value_loss                # minimize\n",
        "            loss -= policy_loss + (entropy_coef * entropy)      # maximize\n",
        "\n",
        "\n",
        "        grads = tape.gradient(loss, self.trainable_variables)\n",
        "        grads, _ = tf.clip_by_global_norm(grads, self.clip_global_norm)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"value_loss\": value_loss,\n",
        "            \"policy_loss\": policy_loss,\n",
        "            \"entropy\": entropy,\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy2XQCt7ZLLl"
      },
      "source": [
        "##Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "fPXpXEgKZFwv"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, agent_id, env,trajectory_length, hist_len=100, e_info=None):\n",
        "        self.agent_id = agent_id\n",
        "        self.env = env\n",
        "        self._state = None\n",
        "        self._trajectory_length = trajectory_length\n",
        "        self._reward = 0\n",
        "        self._reset_trajectory()\n",
        "        self._e_info = e_info\n",
        "\n",
        "\n",
        "    def _reset_trajectory(self):\n",
        "        length = self._trajectory_length\n",
        "        self.trajectory = {\"s\": np.empty((length,)+self.env.observation_space.shape, dtype=self.env.observation_space.dtype),\n",
        "                           \"a\": np.empty((length), dtype=np.int32),\n",
        "                           \"r\": np.empty((length), dtype=np.float32),\n",
        "                           \"p\": np.empty((length, self.env.action_space.n), dtype=np.float32),\n",
        "                           \"v\": np.empty((length), dtype=np.float32),\n",
        "                           \"dones\": np.empty((length), dtype=np.int32)\n",
        "                           }\n",
        "        self.count = 0\n",
        "\n",
        "    def reset_env(self):\n",
        "        self._state = self.env.reset()\n",
        "        return self._state\n",
        "\n",
        "    def step_by_value_and_prob(self, value, logit):\n",
        "        self.trajectory[\"v\"][self.count]=value\n",
        "        self.trajectory[\"p\"][self.count]=logit\n",
        "\n",
        "        def softmax(x):\n",
        "            x = np.exp(x - np.max(x))\n",
        "            return x / np.sum(x)\n",
        "\n",
        "        prob = softmax(logit)\n",
        "        action = np.random.choice( self.env.action_space.n, p=prob)\n",
        "\n",
        "        return self.step(action)\n",
        "\n",
        "    def step(self, action):\n",
        "        state = self._state\n",
        "        next_state, reward, _, _, info, done = self.env.step(action)\n",
        "\n",
        "        self.trajectory[\"s\"][self.count]=state\n",
        "        self.trajectory[\"a\"][self.count]=action\n",
        "        self.trajectory[\"r\"][self.count]=reward\n",
        "        self.trajectory[\"dones\"][self.count]=done\n",
        "\n",
        "        self._reward += reward\n",
        "        if done:\n",
        "            next_state = self.env.reset()\n",
        "            if ATARI_LIVES_KEY in info: # Atari\n",
        "                if \"episode\" in info:\n",
        "                    e_info = info[\"episode\"]\n",
        "                    e_info.update( {\"reward\":self._reward} )\n",
        "                    self._e_info.append(e_info)\n",
        "                    self._reward = 0\n",
        "            else:\n",
        "                self._e_info.append({\"score\":self._reward,\"reward\":self._reward})\n",
        "                self._reward = 0\n",
        "\n",
        "        self._state = next_state\n",
        "        self.count+=1\n",
        "        return self._state\n",
        "\n",
        "    def collect_trajectory(self):\n",
        "        trajectory = self.trajectory\n",
        "        self._reset_trajectory()\n",
        "        return trajectory\n",
        "\n",
        "\n",
        "import math\n",
        "import threading\n",
        "\n",
        "class EpisodesInfo():\n",
        "    def __init__(self):\n",
        "        self._scores = []\n",
        "        self._rewards = []\n",
        "        self._truncated_count = 0\n",
        "        self._episode_count = 0\n",
        "        self._lock = threading.Lock()\n",
        "\n",
        "    def append(self, info):\n",
        "        with self._lock:\n",
        "            self._episode_count += 1\n",
        "            # print(epinfo)\n",
        "            if 'truncated' not in info:\n",
        "                self._scores.append(info[\"score\"])\n",
        "                self._rewards.append(info[\"reward\"])\n",
        "            else:\n",
        "                self._truncated_count += 1\n",
        "\n",
        "    def get_info(self, num_hist=100):\n",
        "        with self._lock:\n",
        "            score_max = 0\n",
        "            score_mean = 0\n",
        "            score_min = 0\n",
        "\n",
        "\n",
        "            scores = self._scores[-num_hist:]\n",
        "            if len(scores)==0:\n",
        "                return { 'episodes': 0, 'reward_mean':0, 'score_mean': 0 }\n",
        "\n",
        "            rewards = self._rewards[-num_hist:]\n",
        "\n",
        "            return {\n",
        "                'episodes': self._episode_count,\n",
        "                'truncated': self._truncated_count,\n",
        "                'score_max': max(scores),\n",
        "                'score_min': min(scores),\n",
        "                'score_mean': sum(scores)/len(scores),\n",
        "                'reward_mean': sum(rewards)/len(rewards),\n",
        "            }\n",
        "\n",
        "class AgentManager():\n",
        "    def __init__(self, num_agents, envs, trajectory_length, hist_len=100):\n",
        "        self._e_info =  EpisodesInfo()\n",
        "        hist_len = math.ceil(hist_len/num_agents)\n",
        "        self._agents = [Agent(i, envs[i],trajectory_length, hist_len=hist_len, e_info=self._e_info) for i in range(num_agents)]\n",
        "        self._states = None\n",
        "\n",
        "    def reset(self):\n",
        "        self._states =  [agent.reset_env() for agent in self._agents]\n",
        "\n",
        "    def step(self, actions):\n",
        "        self._states = [agent.step(action) for action, agent in zip(actions, self._agents)]\n",
        "\n",
        "    def step_by_value_and_prob(self, values, action_probs):\n",
        "        self._states = [agent.step_by_value_and_prob(value, prob) for value, prob, agent in zip(values, action_probs, self._agents)]\n",
        "\n",
        "    def collect_trajectory(self):\n",
        "        return [agent.collect_trajectory() for agent in self._agents]\n",
        "\n",
        "    def get_states(self):\n",
        "        return np.array(self._states, dtype=np.float32)\n",
        "\n",
        "    def get_info(self):\n",
        "        return self._e_info.get_info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by3e2tiUpokY"
      },
      "source": [
        "##Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "mjlmLx0v12rU"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "import os\n",
        "import gym\n",
        "from gym import wrappers\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tqdm import tqdm\n",
        "\n",
        "import datetime\n",
        "def make_network(env_name, obs_shape):\n",
        "    if env_name.startswith('CartPole'):\n",
        "        network = tf.keras.models.Sequential([\n",
        "            layers.Dense(64,activation='relu', input_shape=obs_shape),\n",
        "            layers.Dense(64,activation='relu')\n",
        "        ])\n",
        "    else:\n",
        "        print(obs_shape)\n",
        "        envs = []\n",
        "        network = build_atari_model(obs_shape)\n",
        "    return network\n",
        "\n",
        "import random\n",
        "def set_global_seeds(seed):\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "import time\n",
        "\n",
        "def learn( env_name, envs, num_agents=4, prev_policy=None, gamma=0.99, lam=0.95,\n",
        "          entropy_coef=0.01, value_coef=0.5, pi_clip_range=0.1,\n",
        "          trajectory_length=8, batch_size=None, num_epochs = 1, num_batches=1,\n",
        "          num_frames=10000, lr=1e-4 , test_play_interval=256,\n",
        "          mode='a2c', rand_seed=1234, use_tensorboard=True):\n",
        "\n",
        "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    logdir = \"logs/\" + current_time\n",
        "    if use_tensorboard:\n",
        "        summary_writer = tf.summary.create_file_writer(str(logdir))\n",
        "    else:\n",
        "        summary_writer = None\n",
        "\n",
        "    monitordir = logdir\n",
        "\n",
        "    set_global_seeds(rand_seed)\n",
        "\n",
        "    network = make_network(env_name, envs[0].observation_space.shape)\n",
        "    action_space = envs[0].action_space.n\n",
        "    for i in range(len(envs)):\n",
        "        envs[i].seed(rand_seed)\n",
        "\n",
        "\n",
        "    test_score = 0\n",
        "    use_gae = False\n",
        "    metrics = {}\n",
        "\n",
        "    try:\n",
        "        num_samples = num_agents*trajectory_length\n",
        "\n",
        "        if mode=='ppo':\n",
        "            actor_critic = PPO(network, action_space=action_space,\n",
        "                                entropy_coef=entropy_coef, value_coef=value_coef,\n",
        "                                pi_clip_range=pi_clip_range)\n",
        "            use_gae = True\n",
        "        elif mode=='a2c':\n",
        "            actor_critic = A2C(network, action_space=action_space,\n",
        "                                entropy_coef=entropy_coef, value_coef=value_coef)\n",
        "            use_gae = True\n",
        "            lam = 1.0\n",
        "\n",
        "        if prev_policy:\n",
        "            actor_critic.build((None,84,84,4))\n",
        "            actor_critic.set_weights(prev_policy.get_weights())\n",
        "\n",
        "        frames_per_iteration = trajectory_length*num_agents\n",
        "\n",
        "        if batch_size is None:\n",
        "            batch_size = frames_per_iteration//num_batches\n",
        "\n",
        "\n",
        "        if type(lr) is tuple:\n",
        "            lr_decay = lr[1]**(1/num_frames)\n",
        "            lr = lr[0]\n",
        "        else:\n",
        "            lr_decay = (0.1)**(1/num_frames)\n",
        "\n",
        "        print(mode, num_agents, trajectory_length, num_samples, batch_size, num_batches)\n",
        "        print(env_name,envs[0].get_action_meanings())\n",
        "        print(lr, gamma)\n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam(lr, epsilon=1e-7)\n",
        "        actor_critic.compile(optimizer)\n",
        "\n",
        "        agents = AgentManager(num_agents, envs, trajectory_length)\n",
        "        agents.reset()\n",
        "\n",
        "        next_test_point = test_play_interval\n",
        "\n",
        "        info = None\n",
        "\n",
        "\n",
        "        current_frames = 0\n",
        "        test_score, truncated =play_out(envs[-1], actor_critic, video_fps=12, filename='_recording.avi')\n",
        "        convert_avi_file('_recording.avi', f'{logdir}/{env_name}_{current_frames//1000:05d}_[{test_score}].gif')\n",
        "\n",
        "\n",
        "        print('Initial Score : ', test_score)\n",
        "        best_score= -10000000\n",
        "\n",
        "        pbar = tqdm(total=num_frames)\n",
        "        iter_count = 0\n",
        "        for n in range(0, num_frames, frames_per_iteration):\n",
        "            iter_count+=1\n",
        "\n",
        "            # playout\n",
        "            for _ in range(trajectory_length):\n",
        "                values, action_probs = actor_critic.sample(agents.get_states())\n",
        "                agents.step_by_value_and_prob(values, action_probs)\n",
        "\n",
        "            # collect\n",
        "            trajectories = agents.collect_trajectory()\n",
        "\n",
        "            # calc rewards\n",
        "            next_values, _ = actor_critic.sample(agents.get_states())\n",
        "\n",
        "            (states, actions, discounted_rewards, old_probs, advantages) = [], [], [], [], []\n",
        "            (dones,values) = [], []\n",
        "            for idx, trajectory in enumerate(trajectories):\n",
        "                discounted_Rs = [0.0] * trajectory_length\n",
        "                ADVs = [0.0] * trajectory_length\n",
        "\n",
        "                R = next_values[idx][0]\n",
        "\n",
        "                tmp_rewards = trajectory[\"r\"]\n",
        "                tmp_dones = trajectory[\"dones\"]\n",
        "                if not use_gae:\n",
        "                    for i in reversed(range(trajectory_length)):\n",
        "                        R = tmp_rewards[i] + gamma * (1 - tmp_dones[i]) * R\n",
        "                        discounted_Rs[i] = R\n",
        "                else:\n",
        "                    last_gae = 0.0\n",
        "                    tmp_values = trajectory[\"v\"]\n",
        "                    for i in reversed(range(trajectory_length)):\n",
        "                        mask = 1 - tmp_dones[i]\n",
        "                        value = tmp_values[i]\n",
        "\n",
        "                        delta = tmp_rewards[i] + (mask * gamma * R) - value\n",
        "                        last_gae = delta + (mask * gamma * lam * last_gae)\n",
        "                        ADVs[i] = last_gae\n",
        "                        discounted_Rs[i] = last_gae+value\n",
        "                        R = value\n",
        "\n",
        "                discounted_rewards.append( discounted_Rs )\n",
        "                advantages.append( ADVs )\n",
        "                states.append( trajectory[\"s\"] )\n",
        "                actions.append( trajectory[\"a\"] )\n",
        "                old_probs.append( trajectory[\"p\"] )\n",
        "\n",
        "                values.append( trajectory[\"v\"] )\n",
        "\n",
        "\n",
        "            states = np.concatenate(states, dtype=states[0].dtype)\n",
        "            old_probs = np.concatenate(old_probs, dtype=np.float32)\n",
        "            actions = np.concatenate(actions, dtype=np.int32)\n",
        "            discounted_rewards = np.concatenate(discounted_rewards, dtype=np.float32)\n",
        "            advantages = np.concatenate(advantages, dtype=np.float32)\n",
        "            values = np.concatenate(values, dtype=np.float32)\n",
        "\n",
        "            # Update\n",
        "            optimizer.lr = lr * (lr_decay**n)\n",
        "\n",
        "            metrics = {}\n",
        "            reward_mean = 0.0\n",
        "            for _ in range(num_epochs):\n",
        "                idxes = [idx for idx in range(len(states))]\n",
        "                random.shuffle(idxes)\n",
        "\n",
        "                for i in range(num_batches):\n",
        "                    idx = idxes[i*batch_size:(i+1)*batch_size]\n",
        "\n",
        "                    metrics = actor_critic.train_step(\n",
        "                        ([states[idx], actions[idx], advantages[idx], old_probs[idx], values[idx]],\n",
        "                         discounted_rewards[idx]))\n",
        "\n",
        "            #Output Results\n",
        "\n",
        "            info = agents.get_info()\n",
        "            reward_mean = info['reward_mean']\n",
        "            score_mean = info['score_mean']\n",
        "            values_mean = sum(values)/len(values)\n",
        "\n",
        "            if summary_writer is not None:\n",
        "                with summary_writer.as_default():\n",
        "                    tf.summary.scalar(\"reward_mean\", reward_mean, step=n)\n",
        "                    tf.summary.scalar(\"score_mean\", score_mean, step=n)\n",
        "                    tf.summary.scalar(\"loss\", metrics[\"loss\"], step=n)\n",
        "                    tf.summary.scalar(\"policy_loss\", metrics[\"policy_loss\"], step=n)\n",
        "                    tf.summary.scalar(\"value_loss\", metrics[\"value_loss\"], step=n)\n",
        "                    tf.summary.scalar(\"entropy\", metrics[\"entropy\"], step=n)\n",
        "                    tf.summary.scalar(\"lr\", optimizer.lr, step=n)\n",
        "                    tf.summary.scalar(\"values_mean\", values_mean, step=n)\n",
        "\n",
        "            log_str = ', '.join([f'{k}={v:.4f}' for k,v in metrics.items()])\n",
        "            pbar.set_postfix_str(log_str+ f', score_mean={score_mean:.4f}')\n",
        "            pbar.update(frames_per_iteration)\n",
        "\n",
        "            current_frames = n+frames_per_iteration\n",
        "            if current_frames >= next_test_point*1000 or current_frames>=num_frames:\n",
        "                if best_score < score_mean:\n",
        "                    actor_critic.save_weights(logdir+f'/{env_name}_best_weights.h5')\n",
        "                    best_score=score_mean\n",
        "\n",
        "                next_test_point += test_play_interval\n",
        "\n",
        "                test_score, truncated =play_out(envs[-1], actor_critic, video_fps=12, filename='_recording.avi')\n",
        "                if os.path.exists('_recording.avi'):\n",
        "                    convert_avi_file('_recording.avi', f'{logdir}/{env_name}_{current_frames//1000:05d}_[{test_score}].gif')\n",
        "\n",
        "                info_str = ', '.join([f'{k}={v:.2f}' if isinstance(v, float) else f'{k}={v}' for k,v in info.items()])\n",
        "                print('')\n",
        "                is_truncated = '(truncated)' if truncated else ''\n",
        "                print(f'    {info_str}, test_score={test_score}{is_truncated}')\n",
        "\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "    finally:\n",
        "        pbar.close()\n",
        "        if actor_critic:\n",
        "            actor_critic.save_weights(logdir+f'/{env_name}_last_weights.h5')\n",
        "    return actor_critic\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP-SMo18i0vZ"
      },
      "source": [
        "##Atari"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntuwfAxCmFIT"
      },
      "source": [
        "### Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "zPKBTLQ5jG8Y"
      },
      "outputs": [],
      "source": [
        "from types import FrameType\n",
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "\n",
        "from gym.core import Wrapper\n",
        "from collections import deque\n",
        "\n",
        "import math\n",
        "\n",
        "# https://github.com/MadryLab/implementation-matters\n",
        "class RewardFilter(gym.core.Wrapper):\n",
        "    def __init__(self, env, reward_coef=None, clip=1.0, gamma = None,\n",
        "                 penalty_on_lost=False, penalty_on_no_reward=None):\n",
        "        super().__init__(env)#, new_step_api=False)\n",
        "        self.gamma = gamma\n",
        "        self.clip = clip\n",
        "        self.reward_coef = reward_coef\n",
        "        self.penalty_on_no_reward = penalty_on_no_reward\n",
        "        self.penalty_on_lost = penalty_on_lost\n",
        "        self._n = 0\n",
        "        self._M = 0.0\n",
        "        self._S = 0.0\n",
        "        self._ret = 0.0\n",
        "        self._lives = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state, reward, _, _, info, done = self.env.step(action)\n",
        "        reward, done, info = self._filter(reward, done, info)\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def disable_penalty(self):\n",
        "        self.penalty_on_lost=False\n",
        "        self.penalty_on_no_reward =None\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "\n",
        "        self._ret = 0.0\n",
        "        self.no_reward_count = 0\n",
        "        self._lives = 0\n",
        "        return obs\n",
        "\n",
        "    def _filter(self, reward, done, info):\n",
        "        if self.reward_coef:\n",
        "            if type(self.reward_coef) is tuple:\n",
        "                reward = reward * ( self.reward_coef[0] if reward>0 else self.reward_coef[1])\n",
        "            else:\n",
        "                reward = self.reward_coef * reward\n",
        "\n",
        "        if self.gamma:\n",
        "            self._ret = self._ret * self.gamma + reward\n",
        "            self._push(self._ret)\n",
        "            reward = reward / (self._std() + 1e-8)\n",
        "\n",
        "        if self.clip:\n",
        "            if type(self.clip)==tuple:\n",
        "                reward = np.clip(reward, self.clip[0], self.clip[1])\n",
        "            else:\n",
        "                reward = np.clip(reward, -self.clip, self.clip)\n",
        "\n",
        "        if self.penalty_on_lost:\n",
        "            lives = info[ATARI_LIVES_KEY]\n",
        "            if self._lives!=0 and lives<self._lives:\n",
        "                reward -= 1.0\n",
        "            self._lives = lives\n",
        "\n",
        "        if self.penalty_on_no_reward is not None:\n",
        "            if done:\n",
        "                self.no_reward_count = 0\n",
        "            else:\n",
        "                if reward==0:\n",
        "                    self.no_reward_count += 1\n",
        "                    # print(self.no_reward_count,self.penalty_on_no_reward[0])\n",
        "                    if self.no_reward_count>=self.penalty_on_no_reward[0]:\n",
        "                        reward = self.penalty_on_no_reward[2]\n",
        "                    if done == False and self.penalty_on_no_reward[1] is not None:\n",
        "                        done = self.no_reward_count>=self.penalty_on_no_reward[1]\n",
        "                        if done:\n",
        "                            info['episode']= {'truncated':True}\n",
        "\n",
        "                else:\n",
        "                    self.no_reward_count = 0\n",
        "\n",
        "        return reward, done, info\n",
        "\n",
        "\n",
        "    def _push(self, x):\n",
        "        self._n += 1\n",
        "        if self._n == 1:\n",
        "            self._M = x\n",
        "        else:\n",
        "            oldM = self._M\n",
        "            self._M = oldM + (x - oldM) / self._n\n",
        "            self._S = self._S + (x - oldM) * (x - self._M)\n",
        "\n",
        "    def _mean(self):\n",
        "        return self._M\n",
        "    def _var(self):\n",
        "        return self._S / (self._n - 1) if self._n > 1 else self._M**2\n",
        "    def _std(self):\n",
        "        return math.sqrt(self._var())\n",
        "\n",
        "class ActionFilter(gym.core.Wrapper):\n",
        "    def __init__(self, env, action_meanings=None, action_rewards=None):\n",
        "        super().__init__(env)#, new_step_api=False)\n",
        "        self.mode_name='train'\n",
        "\n",
        "        org_meanings = env.get_action_meanings()\n",
        "        if action_meanings==None:\n",
        "            action_meanings = org_meanings\n",
        "        self.action_rewards = {}\n",
        "        self.act = [0]*len(action_meanings)\n",
        "        for idx, action in enumerate(action_meanings):\n",
        "            self.act[idx] = org_meanings.index(action)\n",
        "            if action_rewards is not None:\n",
        "                if action in action_rewards:\n",
        "                    self.action_rewards[self.act[idx]] = action_rewards[action]\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(len(self.act))\n",
        "        self.action_meanings = action_meanings\n",
        "\n",
        "    def step(self, action):\n",
        "        action = self.act[action]\n",
        "        next_state, reward, terminated, truncated, info, done = self.env.step(action)\n",
        "        if action in self.action_rewards:\n",
        "            reward += self.action_rewards[action]\n",
        "\n",
        "        return obs, reward, terminated, truncated, info, done\n",
        "\n",
        "    def get_action_meanings(self):\n",
        "        return self.action_meanings\n",
        "\n",
        "\n",
        "\n",
        "class AtariWrapper(gym.core.Wrapper):\n",
        "    def __init__(self, env, img_size=(84,84), n_frames=4, n_skips=4,\n",
        "                 fire_on_reset=0, noop_on_reset=(0,15), done_on_lost=True,\n",
        "                 done_on_reward=False, obs_merge=1, score_limit=None):\n",
        "        super().__init__(env)#, #new_step_api=False)\n",
        "        height, width, n_channels = env.observation_space.shape\n",
        "        height, width = img_size\n",
        "        obs_shape = [height, width, n_frames]\n",
        "        self.observation_space = gym.spaces.Box(0.0, 1.0, obs_shape, dtype=np.uint8)\n",
        "        self.img_size = img_size\n",
        "        self.n_frames = n_frames\n",
        "        self.n_skips = n_skips\n",
        "        self.lives = 0\n",
        "        self.true_score = 0\n",
        "        self.episodes = 0\n",
        "        self.score_limit = score_limit\n",
        "        self.fire_on_reset = fire_on_reset\n",
        "        self.act=None\n",
        "        self.mode_name='train'\n",
        "        self.true_reset = True\n",
        "        self.done_on_reward = done_on_reward\n",
        "        self.recording = False\n",
        "        self.obs_merge = obs_merge\n",
        "        self.done_on_lost = done_on_lost\n",
        "        self.noop_on_reset = noop_on_reset\n",
        "\n",
        "    def set_true_reset(self, true_reset=True):\n",
        "        self.true_reset = true_reset\n",
        "\n",
        "    def set_recording(self, flag=True):\n",
        "        self.recording = flag\n",
        "\n",
        "    def get_raw_frames(self):\n",
        "        raw_frames = self.raw_frames\n",
        "        self.raw_frames = []\n",
        "        return raw_frames\n",
        "\n",
        "    def reset(self):\n",
        "        self.no_reward_count = 0\n",
        "        self.frames = deque(maxlen=self.n_frames)\n",
        "\n",
        "        # print('reset', self.true_reset)\n",
        "        if self.true_reset:\n",
        "            self.true_reset = False\n",
        "            self.true_score = 0\n",
        "            self.obs_buffer = deque(maxlen=2)\n",
        "            self.raw_frames = []\n",
        "            self._reset()\n",
        "\n",
        "        for i in range(np.random.randint(low=self.noop_on_reset[0], high=self.noop_on_reset[1], size=1)[0]):\n",
        "            self._step(0)\n",
        "\n",
        "        for i in range(self.fire_on_reset):\n",
        "            self._step(1)\n",
        "\n",
        "\n",
        "        merged_frame = self._merge_frames()\n",
        "        for i in range(self.n_frames):\n",
        "            self.frames.append(merged_frame)\n",
        "\n",
        "        frame = np.concatenate(self.frames, axis = -1)\n",
        "        return  frame\n",
        "\n",
        "\n",
        "    def _reset(self):\n",
        "        obs = self.env.reset()\n",
        "        if self.recording:\n",
        "            self.raw_frames.append(obs)\n",
        "        self.obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "    def _step(self, action):\n",
        "        obs, reward, terminated, truncated, info, dont = super().step(action)\n",
        "        if self.recording:\n",
        "            self.raw_frames.append(obs)\n",
        "        self.obs_buffer.append(obs)\n",
        "        return obs, reward, terminated, truncated, info, dont\n",
        "\n",
        "    def _preprocess_obs(self, obs):\n",
        "        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
        "        obs = cv2.resize(obs, self.img_size)\n",
        "        obs = obs[:, :, None]\n",
        "        return obs\n",
        "\n",
        "    def _merge_frames(self):\n",
        "        if self.obs_merge == 0 or len(self.obs_buffer)==1:\n",
        "            frame = self._preprocess_obs(self.obs_buffer[-1])\n",
        "        elif self.obs_merge == 1:\n",
        "            buf = [self._preprocess_obs(self.obs_buffer[0]), self._preprocess_obs(self.obs_buffer[1])]\n",
        "            frame = np.array(buf).max(axis=0)\n",
        "        elif self.obs_merge == 2:\n",
        "            buf = [self._preprocess_obs(self.obs_buffer[0]), self._preprocess_obs(self.obs_buffer[1])]\n",
        "            frame = ((buf[0].astype(np.float32) + buf[1].astype(np.float32))/2).astype(np.uint8)\n",
        "        return frame\n",
        "\n",
        "    def _step_pool(self,action):\n",
        "\n",
        "        n_skips = self.n_skips\n",
        "\n",
        "        reward = 0.0\n",
        "        for i in range(n_skips):\n",
        "            obs, tmp_reward, done, info = self._step(action)\n",
        "            reward += tmp_reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        merged_frame = self._merge_frames()\n",
        "\n",
        "        return merged_frame, reward, done, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self._step_pool(action)\n",
        "\n",
        "        self.true_score += reward\n",
        "        if self.score_limit is not None:\n",
        "            if self.true_score >= self.score_limit:\n",
        "                done = True\n",
        "\n",
        "\n",
        "        if not done:\n",
        "            if self.done_on_reward:\n",
        "                if reward != 0:\n",
        "                    done = True\n",
        "            lives = info[ATARI_LIVES_KEY]\n",
        "            if self.done_on_lost:\n",
        "                if self.lives!=0 and lives!=self.lives:\n",
        "                    done = True\n",
        "            self.lives = lives\n",
        "        else:\n",
        "            info['episode'] = {\n",
        "                'count':self.episodes+1,\n",
        "                'score':int(self.true_score),\n",
        "            }\n",
        "\n",
        "            self.lives = 0\n",
        "            self.episodes += 1\n",
        "            self.true_reset = True\n",
        "        self.frames.append(obs)\n",
        "\n",
        "        return np.concatenate(self.frames, axis = -1), reward, done, info\n",
        "\n",
        "# test(\"MsPacman\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMHKN_fYcl4m"
      },
      "source": [
        "### Customize Atari Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "bZ_BcRhnckM4"
      },
      "outputs": [
        {
          "ename": "error",
          "evalue": "OpenCV(4.5.4) :-1: error: (-5:Bad argument) in function 'cvtColor'\n> Overload resolution failed:\n>  - src is not a numerical tuple\n>  - Expected Ptr<cv::UMat> for argument 'src'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[36], line 240\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m    239\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m--> 240\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSpaceInvaders\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[36], line 213\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(env_name)\u001b[0m\n\u001b[1;32m    211\u001b[0m envs, _ \u001b[38;5;241m=\u001b[39m build_atari_env_and_param(env_name, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m84\u001b[39m,\u001b[38;5;241m84\u001b[39m))\n\u001b[1;32m    212\u001b[0m env \u001b[38;5;241m=\u001b[39m envs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 213\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mprint\u001b[39m(env\u001b[38;5;241m.\u001b[39mget_action_meanings())\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mprint\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/core.py:323\u001b[0m, in \u001b[0;36mWrapper.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[ObsType, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    322\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment with kwargs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[35], line 40\u001b[0m, in \u001b[0;36mRewardFilter.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 40\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_reward_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "Cell \u001b[0;32mIn[35], line 195\u001b[0m, in \u001b[0;36mAtariWrapper.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfire_on_reset):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 195\u001b[0m merged_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_frames):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mappend(merged_frame)\n",
            "Cell \u001b[0;32mIn[35], line 225\u001b[0m, in \u001b[0;36mAtariWrapper._merge_frames\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_merge_frames\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_merge \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_buffer)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 225\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_merge \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    227\u001b[0m         buf \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_obs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_buffer[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_obs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_buffer[\u001b[38;5;241m1\u001b[39m])]\n",
            "Cell \u001b[0;32mIn[35], line 218\u001b[0m, in \u001b[0;36mAtariWrapper._preprocess_obs\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preprocess_obs\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[0;32m--> 218\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_RGB2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     obs \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size)\n\u001b[1;32m    220\u001b[0m     obs \u001b[38;5;241m=\u001b[39m obs[:, :, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.5.4) :-1: error: (-5:Bad argument) in function 'cvtColor'\n> Overload resolution failed:\n>  - src is not a numerical tuple\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def build_atari_env_and_param(env_name, num_envs, img_size=(84,84)):\n",
        "    gamma = 0.99\n",
        "    envs=[]\n",
        "\n",
        "    for _ in range(num_envs):\n",
        "        env = gym.make(env_name+'NoFrameskip-v4')\n",
        "        if env_name == 'Breakout':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4, fire_on_reset=1, score_limit=864)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_lost=True, penalty_on_no_reward=(200,1000,-0.01))\n",
        "            env = ActionFilter(env, ['NOOP', 'LEFT', 'RIGHT'])\n",
        "            gamma = 0.95\n",
        "        elif env_name == 'SpaceInvaders':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, reward_coef=0.01, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['NOOP', 'FIRE', 'LEFT', 'RIGHT'])\n",
        "        elif env_name == 'CrazyClimber':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, reward_coef=0.001, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['NOOP', 'UP', 'DOWN', 'LEFT', 'RIGHT'])\n",
        "        elif env_name == 'Alien':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, reward_coef=0.01, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['UP', 'DOWN', 'LEFT', 'RIGHT'])\n",
        "        elif env_name == 'Boxing':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_lost=False)\n",
        "            env = ActionFilter(env, [ 'FIRE', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT',])\n",
        "            # env = ActionFilter(env, ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT',])\n",
        "            gamma = 0.95\n",
        "        elif env_name == 'Zaxxon':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['FIRE', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT', ] )\n",
        "        elif env_name == 'Tennis':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4,\n",
        "                               fire_on_reset=0, done_on_reward=True)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_no_reward=(500,500,-1.0))\n",
        "            env = ActionFilter(env, [ 'FIRE', 'UPFIRE', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE'])\n",
        "        elif env_name == 'Seaquest':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, reward_coef=0.01, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN'] )\n",
        "        elif env_name == 'Pong':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4,\n",
        "                               fire_on_reset=1, done_on_reward=True)\n",
        "            env = RewardFilter(env, clip=1.0)\n",
        "            env = ActionFilter(env, [ 'NOOP', 'RIGHT', 'LEFT',] )\n",
        "        elif env_name == 'TimePilot':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, reward_coef=0.001, clip=3.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN', ] )\n",
        "        elif env_name == 'MsPacman':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, reward_coef=0.01, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['UP', 'DOWN', 'LEFT', 'RIGHT'])\n",
        "        elif env_name == 'ChopperCommand':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['FIRE', 'UP', 'DOWN', 'LEFT', 'RIGHT'])\n",
        "        elif env_name == 'Gopher':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, reward_coef=0.01, clip=1.0, penalty_on_lost=True, penalty_on_no_reward=(500,1000,-0.1))\n",
        "            env = ActionFilter(env, ['NOOP', 'FIRE', 'UP', 'LEFT', 'RIGHT'])\n",
        "        elif env_name == 'Bowling':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=30.0)\n",
        "            env = ActionFilter(env, ['FIRE', 'UP'])\n",
        "            gamma = 0.999\n",
        "        elif env_name == 'Kangaroo':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, [ 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN'], {'RIGHT':0.01})\n",
        "        elif env_name == 'Enduro':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=1.0)\n",
        "            env = ActionFilter(env,\n",
        "                ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE'],\n",
        "                {'FIRE':0.01})\n",
        "            gamma = 0.95\n",
        "        elif env_name == 'Freeway':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=1.0)\n",
        "            env = ActionFilter(env, ['NOOP', 'UP'])\n",
        "        elif env_name == 'Gravitar':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=1.0)\n",
        "            env = ActionFilter(env,  ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT'])\n",
        "        elif env_name == 'BeamRider':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env,  ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT'])\n",
        "        elif env_name == 'DoubleDunk':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4, done_on_reward=True)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_no_reward=(500,5000,-0.1))\n",
        "            env = ActionFilter(env,\n",
        "                [ 'NOOP','FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN',  'UPFIRE', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE',])\n",
        "        elif env_name == 'Robotank':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['FIRE', 'UP', 'RIGHT', 'LEFT', 'UPRIGHT', 'UPLEFT'] )\n",
        "            gamma=0.995\n",
        "        elif env_name == 'KungFuMaster':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, reward_coef=0.001, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env,\n",
        "                ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE'],\n",
        "                {'LEFT':0.01})\n",
        "        elif env_name == 'MontezumaRevenge':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['UP', 'RIGHT', 'LEFT', 'DOWN', 'RIGHTFIRE', 'LEFTFIRE'])\n",
        "        elif env_name == 'RoadRunner':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4 )\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env,\n",
        "                ['UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT', 'LEFTFIRE', 'DOWNLEFTFIRE', 'UPLEFTFIRE'],\n",
        "                {'LEFT':0.001})\n",
        "            gamm=0.95\n",
        "        elif env_name == 'Amidar':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['UP', 'RIGHT', 'LEFT', 'DOWN'])\n",
        "            gamma = 0.995\n",
        "        elif env_name == 'Asteroids':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, reward_coef=0.01, clip=1.0, penalty_on_lost=True )\n",
        "            env = ActionFilter(env, ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT'])\n",
        "        elif env_name == 'IceHockey':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4, done_on_reward=True)\n",
        "            env = RewardFilter(env, clip=1.0)\n",
        "            env = ActionFilter(env, ['FIRE', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT', ] )\n",
        "        elif env_name == 'Frostbite':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, reward_coef=0.01, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN'] )\n",
        "        elif env_name == 'FishingDerby':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=(0.0,10.0))\n",
        "            env = ActionFilter(env, ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN',])\n",
        "        elif env_name == 'BattleZone':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN','UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT', ] )\n",
        "        elif env_name == 'WizardOfWor':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_lost=True, penalty_on_no_reward=(500,1000,-0.01))\n",
        "            env = ActionFilter(env, ['FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN', ])\n",
        "            gamma = 0.995\n",
        "        elif env_name == 'BankHeist':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN'] )\n",
        "        elif env_name == 'PrivateEye':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, reward_coef=0.1, clip=10.0, penalty_on_no_reward=(200,5000,-0.01))\n",
        "            env = ActionFilter(env, ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'RIGHTFIRE', 'LEFTFIRE'] )\n",
        "        elif env_name == 'Asterix':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, reward_coef=0.01, clip=1.0, penalty_on_lost=True)\n",
        "            # env = ActionFilter(env, ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN',])\n",
        "        elif env_name == 'NameThisGame':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, reward_coef=0.01, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['NOOP', 'FIRE', 'RIGHT', 'LEFT'])\n",
        "        elif env_name == 'Riverraid':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['NOOP', 'FIRE', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT'])\n",
        "        elif env_name == 'Jamesbond':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT'])\n",
        "        elif env_name == 'Atlantis':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4, score_limit=100000)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_lost=True)\n",
        "        elif env_name == 'Centipede':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, reward_coef=0.01, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN'])\n",
        "        elif env_name == 'Assault':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, reward_coef=0.01, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['UP', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE'])\n",
        "        elif env_name == 'StarGunner':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, reward_coef=0.01, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT',] )\n",
        "        elif env_name == 'DemonAttack':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['NOOP', 'FIRE', 'RIGHT', 'LEFT'])\n",
        "        elif env_name == 'UpNDown':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, reward_coef=0.01, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['NOOP','FIRE', 'UP', 'DOWN'],{'UP':0.1})\n",
        "        elif env_name == 'VideoPinball':\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, reward_coef=0.001, clip=1.0, penalty_on_lost=True)\n",
        "            env = ActionFilter(env, ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN'])\n",
        "        else:\n",
        "            print(\"Default AtariWrapper\")\n",
        "            env = AtariWrapper(env, img_size=img_size, n_skips=4)\n",
        "            env = RewardFilter(env, clip=1.0, penalty_on_lost=True)\n",
        "        envs.append(env)\n",
        "    return envs, gamma\n",
        "\n",
        "\n",
        "# test( env, \".\")\n",
        "from PIL import Image, ImageDraw\n",
        "def test(env_name):\n",
        "    envs, _ = build_atari_env_and_param(env_name, 1, (84,84))\n",
        "    env = envs[0]\n",
        "    obs = env.reset()\n",
        "    print(env.get_action_meanings())\n",
        "    print(env.action_space)\n",
        "    print(env.observation_space)\n",
        "    imgs = []\n",
        "    for i in range(90):\n",
        "        obs, reward, terminated, truncated, info, done = env.step(0)\n",
        "    for n in range(0,16):\n",
        "        # print(obs.shape)\n",
        "        plt.figure(figsize=(16,4))\n",
        "        for i in range(4):\n",
        "            img = obs[:,:,i]\n",
        "            pil_img = Image.fromarray(img)\n",
        "            plt.subplot(1, 4, i+1)\n",
        "            plt.imshow(np.asarray(pil_img), cmap='gray',interpolation='none')\n",
        "            plt.axis('off')\n",
        "        plt.show()\n",
        "        action = np.random.randint(0,env.action_space.n)\n",
        "        action = 0\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        # print(reward, action)\n",
        "        if done:\n",
        "            obs = env.reset()\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore', DeprecationWarning)\n",
        "test(\"SpaceInvaders\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjCHiUOYl1I1"
      },
      "source": [
        "### Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLusAU33jSba"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "def conv(x, filters, kernel_size, strides=1, padding='VALID', norm=None, activation='relu', kernel_initializer='glorot_normal'):\n",
        "    x = layers.Conv2D(filters, kernel_size, strides=strides, padding=padding, use_bias=not norm, kernel_initializer=kernel_initializer)(x)\n",
        "    if norm=='batch':\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    elif norm=='layer':\n",
        "        x = layers.LayerNormalization()(x)\n",
        "    x = layers.Activation(activation)(x)\n",
        "    return x\n",
        "\n",
        "def dense(x, filters, norm=None, activation='relu', kernel_initializer='glorot_normal'):\n",
        "    x = layers.Dense(filters, use_bias=not norm, kernel_initializer=kernel_initializer)(x)\n",
        "    if norm=='batch':\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    elif norm=='layer':\n",
        "        x = layers.LayerNormalization()(x)\n",
        "    x = layers.Activation(activation)(x)\n",
        "    return x\n",
        "\n",
        "def build_atari_model(in_shape):\n",
        "    inputs = layers.Input(in_shape)\n",
        "    x = layers.Lambda( lambda x: x/255.0)(inputs)\n",
        "\n",
        "    kernel_initializer = tf.initializers.Orthogonal(np.sqrt(2))\n",
        "    activation = 'relu'\n",
        "    norm = None\n",
        "    if x.shape[1]<84:\n",
        "        pad = (84-x.shape[1])//2\n",
        "        x = tf.pad(x, [[0,0],[pad,pad],[pad,pad],[0,0]] )\n",
        "    x = conv(x, 32, (8, 8), strides=4,\n",
        "            activation=activation, kernel_initializer=kernel_initializer, norm=norm)\n",
        "    x = conv(x, 64, (4, 4), strides=2,\n",
        "            activation=activation, kernel_initializer=kernel_initializer, norm=norm)\n",
        "    x = conv(x, 64, (3, 3), strides=1,\n",
        "            activation=activation, kernel_initializer=kernel_initializer, norm=norm)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = dense(x, 512,\n",
        "            activation=activation, kernel_initializer=kernel_initializer, norm=norm)\n",
        "\n",
        "    network = models.Model(inputs, x)\n",
        "    return network\n",
        "\n",
        "\n",
        "\n",
        "network = build_atari_model((84,84,4))\n",
        "network.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se51b1oIl9mj"
      },
      "source": [
        "##Play Out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF-5QnUsCisz"
      },
      "outputs": [],
      "source": [
        "from traitlets.traitlets import DottedObjectName\n",
        "# !pip install gym-notebook-wrapper\n",
        "# import gnwrapper\n",
        "import gym\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "import os\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "from IPython.display import display,HTML\n",
        "matplotlib.interactive(False)\n",
        "\n",
        "#https://github.com/horoiwa/deep_reinforcement_learning_gallery/blob/master/MuZero/visualize.py\n",
        "\n",
        "def draw_frame_and_info(env, rgb, info, draw_type='raw'):\n",
        "    step, action, reward, score, value, lives = info\n",
        "\n",
        "    action_name = env.get_action_meanings()[action]\n",
        "    img_frame = Image.fromarray(rgb)\n",
        "\n",
        "    if draw_type=='raw':\n",
        "        return img_frame\n",
        "\n",
        "    if draw_type=='wide':\n",
        "        resized_img = img_frame.resize((img_frame.width*2, img_frame.height), Image.NEAREST)\n",
        "        return resized_img\n",
        "\n",
        "    img_desc = Image.new('RGB', (160, img_frame.height), color=\"black\")\n",
        "    fnt = ImageFont.truetype(\"LiberationSans-Regular.ttf\", 12)\n",
        "    fnt_sm = ImageFont.truetype(\"LiberationSans-Regular.ttf\", 10)\n",
        "\n",
        "    pl = 10\n",
        "    pb = 20\n",
        "\n",
        "    v = str(round(value, 2))\n",
        "\n",
        "    draw = ImageDraw.Draw(img_desc)\n",
        "    draw.fontmode = '1'\n",
        "    draw.text((pl, 20+pb*0), f\"Step: {step:,}\", font=fnt, fill=\"white\")\n",
        "    draw.text((pl, 20+pb*1), f\"Lives: {int(lives):,}\", font=fnt, fill=\"white\")\n",
        "    draw.text((pl, 20+pb*2), f\"Score: {int(score):,}\", font=fnt, fill=\"white\")\n",
        "    draw.text((pl, 20+pb*3), f\"Action: {action_name}\", font=fnt, fill=\"white\")\n",
        "    draw.text((pl, 20+pb*4), f\"Reward: {reward:.4f}\", font=fnt, fill=\"white\")\n",
        "    draw.text((pl, 20+pb*5), f\"V(s): {v}\", font=fnt, fill=\"white\")\n",
        "\n",
        "\n",
        "    img_bg = Image.new(\n",
        "        'RGB', (img_frame.width + img_desc.width, img_frame.height))\n",
        "\n",
        "    img_bg.paste(img_frame, (0, 0))\n",
        "    img_bg.paste(img_desc, (img_frame.width, 0))\n",
        "\n",
        "    return img_bg\n",
        "\n",
        "\n",
        "def play_out(env, policy, filename='_recording.png', verbose=0,\n",
        "              video_fps=12,\n",
        "              deterministic=False, draw_type='info'):\n",
        "    ims = []\n",
        "    is_atari = False\n",
        "    if hasattr(env, 'set_recording'):\n",
        "        is_atari = True\n",
        "        env.set_recording(True)\n",
        "\n",
        "    # if hasattr(env, 'start_recording'):\n",
        "    #     env.start_recording(filename, 30.0)\n",
        "    if hasattr(env, 'set_true_reset'):\n",
        "        env.set_true_reset()\n",
        "\n",
        "    if hasattr(env, 'disable_penalty'):\n",
        "        env.disable_penalty()\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    score = 0\n",
        "    done = False\n",
        "    step_count = 0\n",
        "    no_reward_count = 0\n",
        "\n",
        "    frames = []\n",
        "    frame_count=0\n",
        "    video_skip_num = 60//video_fps\n",
        "\n",
        "\n",
        "    truncated = False\n",
        "    video = None\n",
        "    lives = 0\n",
        "\n",
        "    while not done:\n",
        "        if policy==None:\n",
        "            action = np.random.randint(0,env.action_space.n)\n",
        "            value = 0.0\n",
        "            prob = [0.0]\n",
        "        else:\n",
        "            values, logits = policy.sample(np.expand_dims(state,0))\n",
        "\n",
        "            def softmax(x):\n",
        "                x = np.exp(x - np.max(x))\n",
        "                return x / np.sum(x)\n",
        "\n",
        "            prob = softmax(logits[0])\n",
        "\n",
        "            if deterministic:\n",
        "                action = np.argmax(logits[0])\n",
        "            else:\n",
        "                action = np.random.choice( len(prob), p=prob)\n",
        "            value = values[0][0]\n",
        "\n",
        "        next_state, reward, terminated, truncated, info, dont = env.step(action)\n",
        "        if any(info):\n",
        "            lives  = info[ATARI_LIVES_KEY] if ATARI_LIVES_KEY in info else 0\n",
        "\n",
        "        if is_atari:\n",
        "            score = env.true_score\n",
        "            frame_info = (step_count, action, reward, score, value, lives)\n",
        "            for frame in env.get_raw_frames():\n",
        "                frames.append(frame)\n",
        "                if len(frames)==video_skip_num:\n",
        "                    frames = []\n",
        "                    rgb = draw_frame_and_info(env, frame, frame_info , draw_type)\n",
        "                    rgb = np.array(rgb)[:,:,::-1]\n",
        "                    if video==None:\n",
        "                        fourcc = cv2.VideoWriter_fourcc('p','n','g', ' ')\n",
        "                        video  = cv2.VideoWriter( filename, fourcc, video_fps, (rgb.shape[1],rgb.shape[0]) )\n",
        "                    video.write(rgb)\n",
        "                frame_count+=1\n",
        "        else:\n",
        "            score = score+reward\n",
        "\n",
        "        if verbose>=2:\n",
        "            if reward!=0 or done:\n",
        "                print(step_count,reward, int(score), action, done, info)\n",
        "\n",
        "        if done:\n",
        "            if any(info):\n",
        "                if 'TimeLimit.truncated' in info: # max step\n",
        "                    break\n",
        "                elif 'episode' in info:\n",
        "                    if 'truncated' in info['episode']:\n",
        "                        print('truncated')\n",
        "                    if 'score' in info['episode']:\n",
        "                        score = info['episode']['score']\n",
        "                    break\n",
        "                done = False\n",
        "                env.reset()\n",
        "        else:\n",
        "            state = next_state\n",
        "\n",
        "        step_count+=1\n",
        "        if step_count>=10000:\n",
        "            truncated=True\n",
        "            break\n",
        "\n",
        "\n",
        "    if verbose!=0:\n",
        "        print( f'step_count={step_count}, frames={len(frames)}, score={score}' )\n",
        "\n",
        "    if is_atari and filename:\n",
        "        if len(frames)!=0:\n",
        "            rgb = draw_frame_and_info(env, frames[-1], frame_info , draw_type)\n",
        "            rgb = np.array(rgb)[:,:,::-1]\n",
        "        for i in range(video_fps*4):\n",
        "            video.write(rgb)\n",
        "        video.release()\n",
        "\n",
        "        if verbose!=0:\n",
        "            fsize = os.path.getsize(filename)\n",
        "            print( f'{filename} size={fsize:,}, frames={frame_count:,}')\n",
        "\n",
        "    return int(score), truncated\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def convert_avi_file(in_filename, out_filename):\n",
        "    if out_filename.endswith('.gif'):\n",
        "        ffmpg_options = '-filter_complex \"[0:v]palettegen=reserve_transparent=0[pal];[0:v][pal]paletteuse=dither=none\"'\n",
        "    elif out_filename.endswith('.png'):\n",
        "        ffmpg_options = '-filter_complex \"[0:v]palettegen=reserve_transparent=0[pal];[0:v][pal]paletteuse=dither=none\" -f apng'\n",
        "    else:\n",
        "        ffmpg_options = '-vb 500k'\n",
        "    cmdline = f'ffmpeg -y -i {in_filename} {ffmpg_options} {out_filename}'\n",
        "    try:\n",
        "        result = subprocess.run(cmdline, shell=True, check=True,\n",
        "                                stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
        "                                universal_newlines=True)\n",
        "        # print(result)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "env_name = 'VideoPinball'\n",
        "\n",
        "envs, gamma = build_atari_env_and_param(env_name,1)\n",
        "model = make_network(env_name, envs[0].observation_space.shape)\n",
        "print(envs[0].get_action_meanings())\n",
        "print(envs[0].action_space.n)\n",
        "action_space = envs[0].action_space.n\n",
        "\n",
        "policy = PPO(model, action_space=action_space)\n",
        "optimizer = tf.keras.optimizers.Adam(0.1, epsilon=1e-7)\n",
        "policy.compile(optimizer)\n",
        "policy.build((1,84,84,4))\n",
        "\n",
        "\n",
        "# score, truncated = play_out( envs[0], policy, verbose=1, video_fps=12, deterministic=False, filename='_recording.avi')\n",
        "# convert_avi_file('_recording.avi', 'test.gif')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAnW8kZibQEe"
      },
      "source": [
        "#訓練"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tPFrX-V9wBo"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9vl2kfyt3JY"
      },
      "source": [
        "## TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeO2V0HYx76C"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWU_AlxHt6rE"
      },
      "source": [
        "## 実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZKrSYEp3uAv"
      },
      "outputs": [],
      "source": [
        "#@title  { form-width: \"300px\" }\n",
        "\n",
        "env_name = 'Breakout' #@param ['Amidar','Alien','Asteroids','Assault','Asterix','Atlantis','BattleZone','BeamRider','Boxing','Breakout','Bowling','BankHeist','Centipede','CrazyClimber','ChopperCommand','DoubleDunk','DemonAttack','Enduro','FishingDerby','Freeway','Frostbite','Gravitar','Gopher','IceHockey', 'Jamesbond', 'Kangaroo','KungFuMaster','MsPacman','MontezumaRevenge','NameThisGame', 'Pitfall','Pong','PrivateEye','Qbert','Riverraid','Robotank','RoadRunner','SpaceInvaders','Seaquest','TimePilot','Tennis','UpNDown','VideoPinball','WizardOfWor','Zaxxon']\n",
        "\n",
        "#\n",
        "# PPO\n",
        "#\n",
        "num_agents = 8\n",
        "lr = 2.5e-4\n",
        "envs, gamma = build_atari_env_and_param(env_name,num_agents+1)\n",
        "policy = learn( mode='ppo', prev_policy=None, env_name=env_name, envs=envs,\n",
        "               lr=lr, gamma=gamma,num_frames=10*1000*1000,\n",
        "               num_agents=num_agents, trajectory_length=128,\n",
        "               batch_size=None, num_epochs=4, num_batches=8,\n",
        "               entropy_coef=0.01, value_coef=0.5, pi_clip_range=0.1,\n",
        "               test_play_interval=250, use_tensorboard=True, rand_seed=123)\n",
        "#\n",
        "# A2C\n",
        "#\n",
        "# num_agents = 16\n",
        "# lr = 1e-3\n",
        "# envs, gamma = build_atari_env_and_param(env_name,num_agents+1)\n",
        "# policy = learn( mode='a2c', prev_policy=None, env_name=env_name, envs=envs,\n",
        "#                 lr=lr, gamma=gamma,num_frames=10*1000*1000,\n",
        "#                 num_agents=num_agents, trajectory_length=5,\n",
        "#                 batch_size=None, num_epochs=1, num_batches=1,\n",
        "#                 entropy_coef=0.01, value_coef=0.5,\n",
        "#                 test_play_interval=250, use_tensorboard=True, rand_seed=123)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
